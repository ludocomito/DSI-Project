{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning project - Dataset Preprocessing\n",
        "### Ludovico Comito - Matr. 1837155\n",
        "### Giulio Fedeli - Matr. 1873677\n",
        "### Lorenzo Cirone - Matr. 1930811"
      ],
      "metadata": {
        "id": "34YVLtEyAUXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing\n",
        "The source dataset we use for our experiments is MS Marco Document ranking dataset. All the original files to download can be found inside [this github repo](https://github.com/microsoft/msmarco/blob/master/Datasets.md) under the \"document ranking dataset\" paragraph.\n",
        "The msmarco-docs.tsv file (download [here](https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz)) is a 22gb file containing the all the dataset's documents and contains the columns [docid, url, title, body]. \\\\\n",
        "Each split (train, validation and test) is essentially made of two files: one tsv containing queries and their corresponding query id (qid) and one top 100 file that maps each query id to 100 ranked docids. \\\\\n",
        "As the original dataset contains millions of documents, we decided to extract 8k train queries, 2k validation queries and 2k test queries. In orther to further reduce the number of documents to load, we reduce the rankings to the first 10 relevant documents instead of the first 100.\n",
        "\\\\\n",
        "The final datasets will look like this:\n",
        "\n",
        "\n",
        "*   A training dataset that contains all the documents to be indexed plus the training queries (to train the model in a multitask fashion).\n",
        "*   Validation and Test datasets containing the queries and the file that maps queries to the ids of the first 10 relevant documents (labels).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5E8LIiyVpO45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the training corpus is very large (22gb), we will use the dask library that allows to load the corpus as a dataframe and split it  in chunks that can fit into memory."
      ],
      "metadata": {
        "id": "Mr4gJNszt86X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQsJmHpNpNtx"
      },
      "outputs": [],
      "source": [
        "!pip install dask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "fd9DXPeTue9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: load and sample queries"
      ],
      "metadata": {
        "id": "xWu-KLUwvKP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load queries files and rename the columns\n",
        "train_queries_path = 'msmarco-doctrain-queries.tsv'\n",
        "train_queries = pd.read_csv(train_queries_path, sep = '\\t')\n",
        "train_queries.columns = ['qid', 'query']\n",
        "\n",
        "val_queries_path = 'msmarco-docdev-queries.tsv'\n",
        "val_queries = pd.read_csv(val_queries_path, sep = '\\t')\n",
        "val_queries.columns = ['qid', 'query']\n",
        "\n",
        "test_queries_path = 'docleaderboard-queries.tsv'\n",
        "test_queries = pd.read_csv(test_queries_path, sep = '\\t')\n",
        "test_queries.columns = ['qid', 'query']"
      ],
      "metadata": {
        "id": "S6K0KeCPuwM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 8k queries for train, 2k for val and 2k for validation\n",
        "train_queries = train_queries.sample(8000, random_state = 42).reset_index(drop = True)\n",
        "val_queries = val_queries.sample(2000, random_state = 42).reset_index(drop = True)\n",
        "test_queries = test_queries.sample(2000, random_state = 42).reset_index(drop = True)\n",
        "\n",
        "print(f'train queries: {train_queries.shape[0]}')\n",
        "print(f'val queries: {val_queries.shape[0]}')\n",
        "print(f'test queries: {val_queries.shape[0]}')"
      ],
      "metadata": {
        "id": "z_72zr7muyb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: take the corresponding ranked documents\n",
        "The top100 documents file contain the top 100 documents ranked for each query according to their relevance with the query. The document at rank 1 is most relevant to the query and the document at 100 is least relevant among the 100 documents. Since we have already reduced the number of queries, letâ€™s reduce this one too."
      ],
      "metadata": {
        "id": "t7908bBUvF1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load top 100 tsvs\n",
        "train_top_100_path = 'dataset/train/msmarco-doctrain-top100'\n",
        "\n",
        "train_top100 = pd.read_table(train_top_100_path, delimiter=' ', header = None)\n",
        "train_top100.columns = ['qid', 'Q0', 'docid', 'rank', 'score', 'runstring']\n",
        "print('Shape before resizing=>',train_top100.shape)\n",
        "train_top100.head()\n",
        "\n",
        "# Reducing train_top100 for training\n",
        "training_ranked100 = train_top100[train_top100['qid'].isin(train_queries['qid'].unique())].reset_index(drop=True)\n",
        "print('Shape after resizing=>', training_ranked100.shape)\n",
        "training_ranked100.head()"
      ],
      "metadata": {
        "id": "jzPRVOc3vHAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_top_100_path = 'dataset/val/msmarco-docdev-top100'\n",
        "\n",
        "val_top100 = pd.read_table(val_top_100_path, delimiter = ' ',header=None)\n",
        "val_top100.columns = ['qid','Q0','docid','rank','score','runstring']\n",
        "print('Shape before resizing=>',val_top100.shape)\n",
        "val_top100.head()\n",
        "\n",
        "val_ranked100 = val_top100[val_top100['qid'].isin(val_queries['qid'].unique())].reset_index(drop=True)\n",
        "print('Shape after resizing=>',val_ranked100.shape)\n",
        "val_ranked100.head()"
      ],
      "metadata": {
        "id": "zm1H9rdJvSpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_top_100_path = 'dataset/test/docleaderboard-top100.tsv'\n",
        "\n",
        "test_top100 = pd.read_table(test_top_100_path, delimiter = ' ',header=None)\n",
        "test_top100.columns = ['qid','Q0','docid','rank','score','runstring']\n",
        "print('Shape before resizing=>',test_top100.shape)\n",
        "test_top100.head()\n",
        "\n",
        "test_ranked100 = test_top100[test_top100['qid'].isin(test_queries['qid'].unique())].reset_index(drop=True)\n",
        "print('Shape after resizing=>',test_ranked100.shape)\n",
        "test_ranked100.head()"
      ],
      "metadata": {
        "id": "0Th4YM6svWxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_ranked_documents(ranking_document, queries):\n",
        "    '''\n",
        "    Filters out only the queries that have been subsamples and takes\n",
        "    the first 10 document rankings for each query.\n",
        "    '''\n",
        "    # filter out the rankings for queries that are not in the current query set\n",
        "    print('Original shape=>',ranking_document.shape)\n",
        "    reduced_ranked_documents = ranking_document[ranking_document['qid'].isin(queries['qid'].unique())].reset_index(drop=True)\n",
        "    print('Shape after filtering=>',ranking_document.shape)\n",
        "\n",
        "    # take the first 10 documents for each query\n",
        "    rel = list(range(1,11))\n",
        "    reduced_ranked_documents['rel'] = reduced_ranked_documents['rank'].apply(lambda x: 1 if x in rel else np.nan)\n",
        "    reduced_result=reduced_ranked_documents.dropna()\n",
        "\n",
        "    print('Shape after reduction=>', reduced_result.shape)\n",
        "\n",
        "\n",
        "    return reduced_result"
      ],
      "metadata": {
        "id": "oOR6ZB6DvqVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reducing train_top100')\n",
        "train_top_10 = reduce_ranked_documents(train_top100, train_queries)\n",
        "\n",
        "print('Reducing val_top100')\n",
        "val_top_10 = reduce_ranked_documents(val_top100, val_queries)\n",
        "\n",
        "print('Reducing test_top100')\n",
        "test_top_10 = reduce_ranked_documents(test_top100, test_queries)"
      ],
      "metadata": {
        "id": "Cs7Skgptv522"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save pickled splits"
      ],
      "metadata": {
        "id": "nSCcVznlwhUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_queries_path = 'processed_train_queries.pkl'\n",
        "train_top_10_path = 'train_top_10.pkl'\n",
        "\n",
        "val_queries_path = 'processed_val_queries.pkl'\n",
        "val_top_10_path = 'processed_val_ranked_top100.pkl'\n",
        "\n",
        "test_queries_path = 'processed_test_queries.pkl'\n",
        "test_top_10_path = 'processed_test_ranked_top100.pkl'\n",
        "\n",
        "train_queries.to_pickle(train_queries_path)\n",
        "training_ranked100.to_pickle(train_top_10_path)\n",
        "\n",
        "val_queries.to_pickle(val_queries_path)\n",
        "val_ranked100.to_pickle(val_top_10_path)\n",
        "\n",
        "test_queries.to_pickle(test_queries_path)\n",
        "test_ranked100.to_pickle(test_top_10_path)"
      ],
      "metadata": {
        "id": "LxRVFXk3wh4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: reduce corpus\n",
        "Take only the relevant documents from the main 22GB file"
      ],
      "metadata": {
        "id": "OA4zy-uTwjSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = dd.read_table('dataset/msmarco-docs.tsv', blocksize = 100e6, header = None)\n",
        "df.columns = ['docid','url','title','body']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6Q7Ok-ocwk1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus(result, df):\n",
        "    '''\n",
        "    Filters out the documents present in the split.\n",
        "    '''\n",
        "    unique_docid = result['docid'].unique()\n",
        "    condition = df['docid'].isin(unique_docid)\n",
        "    corpus = df[condition].reset_index(drop = True)\n",
        "    corpus = corpus.drop(columns = 'url')\n",
        "    print('Number of Rows=>',len(corpus))\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "GBSDJrwOwm8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus = create_corpus(train_top_10, df)\n",
        "training_corpus_df = training_corpus.compute()\n",
        "training_corpus_df.to_pickle('training_corpus_df.pkl')\n",
        "\n",
        "val_corpus = create_corpus(val_top_10, df)\n",
        "val_corpus_df = val_corpus.compute()\n",
        "val_corpus_df.to_pickle('val_corpus_df.pkl')\n",
        "\n",
        "test_corpus = create_corpus(test_top_10, df)\n",
        "test_corpus.head()\n",
        "test_corpus_df = test_corpus.compute()\n",
        "test_corpus_df.to_pickle('test_corpus_df.pkl')"
      ],
      "metadata": {
        "id": "vHraiSAhwoh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Semantic clustering\n",
        "As pointed out by the DSI paper, reassigning IDs based on semantic clustering (documents within the same cluster will have similar docids) can greatly improve the performance of the model, expecially when paired with beam search as a decoding strategy. \\\\\n",
        "The following code implements semantic clustering by first creating the embeddings for each document and applying a k-means algorithm to clusterize and assign semantic ids."
      ],
      "metadata": {
        "id": "GlM52U2y-7sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create the embeddings for our documents, we utilize a sentence transformer from the HuggingFace library. In particular, we utilize *distilroberta-base-nli-matryoshka-256* which is a distilled Roberta based model that we found being a good tradeoff between the dimensionality of the produced embeddings (256 dimensions vs 768 of BERT) and the quality of the obtained representations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oARvwvVEATr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "Dmf9K0uEJOFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "KEOhHQNRw7kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_dict = {\n",
        "    'train_corpus_path': 'training_corpus_df.pkl', # train corpus to be embedded\n",
        "    'val_corpus_path': 'val_corpus_df.pkl', # val corpus to be embedded\n",
        "    'test_corpus_path': 'test_corpus_df.pkl' # test corpus to be embedded\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "QkPyJYV9wqUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pickle(filepath):\n",
        "    '''\n",
        "    Loads and returns a pickle file stored in filepath.\n",
        "    '''\n",
        "    with open(filepath, 'rb') as file:\n",
        "        loaded_file = pickle.load(file)\n",
        "        return loaded_file\n",
        "\n",
        "\n",
        "def export_pickle(element, filepath):\n",
        "    '''\n",
        "    Stores the passed elements as a pickle file.\n",
        "    '''\n",
        "    with open(filepath, 'wb') as file:\n",
        "        pickle.dump(element, file)"
      ],
      "metadata": {
        "id": "BAHSg_4bwsIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus_df = load_pickle(files_dict['train_corpus_path'])\n",
        "val_corpus_df = load_pickle(files_dict['val_corpus_path'])\n",
        "test_corpus_df = load_pickle(files_dict['test_corpus_path'])"
      ],
      "metadata": {
        "id": "9xE2Z0gCBUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We unify the title and body of each documents for all the splits.\n",
        "train_corpus_df['document'] = train_corpus_df['title'] + ' ' + train_corpus_df['body']\n",
        "train_unified_df = train_corpus_df[['docid', 'document']]\n",
        "\n",
        "val_corpus_df['document'] = val_corpus_df['title'] + ' ' + val_corpus_df['body']\n",
        "val_unified_df = val_corpus_df[['docid', 'document']]\n",
        "\n",
        "test_corpus_df['document'] = test_corpus_df['title'] + ' ' + test_corpus_df['body']\n",
        "test_unified_df = test_corpus_df[['docid', 'document']]"
      ],
      "metadata": {
        "id": "UxTwXj5UDPXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the sentence transformer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sentence_transformer = SentenceTransformer('tomaarsen/distilroberta-base-nli-matryoshka-256').to(device)"
      ],
      "metadata": {
        "id": "3zOtldE8Bm1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings_dict(corpus_df, sentence_transformer):\n",
        "    '''\n",
        "    Given a corpus (a dataframe with docid and body columns), this function\n",
        "    utilizes the sentence transformer to embed the document and store it\n",
        "    inside a dictionary with its corresponding key being the original docid.\n",
        "    '''\n",
        "    embeddings_dict = {}\n",
        "\n",
        "    for i in range(len(corpus_df)):\n",
        "        current_docid = corpus_df['docid'].iloc[i]\n",
        "        current_document = corpus_df['body'].iloc[i]\n",
        "\n",
        "        if type(current_document) is str:\n",
        "            embedding = sentence_transformer.encode(current_document)\n",
        "            embeddings_dict[current_docid] = embedding\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return embeddings_dict"
      ],
      "metadata": {
        "id": "O1yQFrzqBtKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionaries mapping docid to embeddings for train, val and test splits\n",
        "train_embeddings_dict = create_embeddings_dict(train_corpus_df)\n",
        "val_embeddings_dict = create_embeddings_dict(val_corpus_df)\n",
        "test_embeddings_dict = create_embeddings_dict(test_corpus_df)"
      ],
      "metadata": {
        "id": "EwAlN9rhCKlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge all the embeddings into a single dict\n",
        "total_embeddings_dict = {}\n",
        "\n",
        "total_embeddings_dict.update(train_embeddings_dict)\n",
        "total_embeddings_dict.update(val_embeddings_dict)\n",
        "total_embeddings_dict.update(test_embeddings_dict)"
      ],
      "metadata": {
        "id": "X8BKzQtTCRwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export a pickle checkpoint containing all the embeddings\n",
        "export_pickle(total_embeddings_dict, 'total_embeddings_dict.pkl')"
      ],
      "metadata": {
        "id": "UED0lX2dCYVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement the algorithm that clusters and reassigns the docids: the algorithm recursively creates clusters of embeddings using k-means. The final result will be a dictionary that maps the original docids to the newly created semantic ids."
      ],
      "metadata": {
        "id": "qsJXZyNOCm3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def cluster_documents(docs_embeddings):\n",
        "    '''\n",
        "    Clusters documents into k clusters using k-means on the embeddings.\n",
        "    '''\n",
        "    embeddings = list(docs_embeddings.values())\n",
        "    kmeans = KMeans(n_clusters = 10, random_state = 0).fit(embeddings)\n",
        "    clusters = {i: [] for i in range(10)}\n",
        "    for docid, label in zip(docs_embeddings.keys(), kmeans.labels_):\n",
        "        clusters[label].append(docid)\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def generate_semantic_ids(docs_embeddings, c = 100, prefix = ''):\n",
        "    '''\n",
        "    Recursively generates semantically structured identifiers with mapping.\n",
        "    '''\n",
        "    if len(docs_embeddings) == 0:\n",
        "        return {}\n",
        "\n",
        "    # Cluster the documents based on their embeddings\n",
        "    clusters = cluster_documents(docs_embeddings)\n",
        "    new_doc_ids = {}\n",
        "\n",
        "    for i in range(10):\n",
        "        cluster_docids = clusters[i]\n",
        "        cluster_embeddings = {docid: docs_embeddings[docid] for docid in cluster_docids}\n",
        "\n",
        "        if len(cluster_embeddings) > c:\n",
        "            # Recursively cluster further\n",
        "            Jrest = generate_semantic_ids(cluster_embeddings, c, prefix = f\"{prefix}{i}\")\n",
        "        else:\n",
        "            # Assign unique identifier within this cluster\n",
        "            Jrest = {docid: f\"{prefix}{i}{j}\" for j, docid in enumerate(cluster_docids)}\n",
        "\n",
        "        new_doc_ids.update(Jrest)\n",
        "\n",
        "    return new_doc_ids"
      ],
      "metadata": {
        "id": "WUQVb5CZChzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docid_to_semantic_map = generate_semantic_ids(total_embeddings_dict)"
      ],
      "metadata": {
        "id": "OzI3vmt4FUB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_pickle(docid_to_semantic_map, 'docid_to_semantic_map.pkl')"
      ],
      "metadata": {
        "id": "kQ60cGIHDH3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use the map function of the Pandas library to map every docid in the dataset to its corresponding semantic id."
      ],
      "metadata": {
        "id": "EbSDpakwDaVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_docid_to_semantic(docid):\n",
        "    return docid_to_semantic_map.get(docid, None)\n"
      ],
      "metadata": {
        "id": "dm0g8XJEFgfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map docids to semantic ids and create a new column\n",
        "mapped_train_top_10 = train_top_10.copy()\n",
        "mapped_train_top_10['semantic_id'] = mapped_train_top_10['docid'].map(map_docid_to_semantic)\n",
        "\n",
        "mapped_val_top_10 = val_top_10.copy()\n",
        "mapped_val_top_10['semantic_id'] = mapped_val_top_10['docid'].map(map_docid_to_semantic)\n",
        "\n",
        "mapped_test_top_10 = test_top_10.copy()\n",
        "mapped_test_top_10['semantic_id'] = mapped_test_top_10['docid'].map(map_docid_to_semantic)"
      ],
      "metadata": {
        "id": "CYvHrEl_DKZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At training time, the model will need to index all the corpuses of train, validation and test, while it will be trained only on the training queries."
      ],
      "metadata": {
        "id": "Qw-qnNAgGt3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remap docids of the various corpuses and unify them into a single dataset\n",
        "mapped_train_corpus = train_corpus_df.copy()\n",
        "mapped_train_corpus['semantic_id'] = mapped_train_corpus['docid'].map(map_docid_to_semantic)\n",
        "\n",
        "mapped_val_corpus = val_corpus_df.copy()\n",
        "mapped_val_corpus['semantic_id'] = mapped_val_corpus['docid'].map(map_docid_to_semantic)\n",
        "\n",
        "mapped_test_corpus = test_corpus_df.copy()\n",
        "mapped_test_corpus['semantic_id'] = mapped_test_corpus['docid'].map(map_docid_to_semantic)\n",
        "\n",
        "full_corpus_df = pd.concat([mapped_train_corpus, mapped_val_corpus, mapped_test_corpus], ignore_index=True)"
      ],
      "metadata": {
        "id": "e0O9ZKvLDnxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final step, it's time to process the queries. Inside the training dataset, we will add to the corpus the train queries, where each query will have as a label the id of the document ranked as number one. \\\\\n",
        "However, for validation and test, we want to evaluate the capability of the model at producing a ranking of 10 relevant documents for that query. In that case, we will aim at making a dataframe where each query is mapped to the 10 most relevant ids."
      ],
      "metadata": {
        "id": "Rd7l3n8rKwUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the top 1 ranked id for each train query, and create a dictionary that maps each query to the docid.\n",
        "query_to_semantic_docid_map = {}\n",
        "for i in range(len(train_queries)):\n",
        "    current_row = train_queries.iloc[i]\n",
        "    current_qid = current_row['qid']\n",
        "    current_query = current_row['query']\n",
        "\n",
        "    current_semantic_docid_top10 = train_top10[train_top10['qid'] == current_qid]\n",
        "    if len(current_semantic_docid_top10) > 0:\n",
        "        current_semantic_docid = current_semantic_docid_top10.iloc[0]['semantic_id'] # take the docid with rank 1\n",
        "        query_to_semantic_docid_map[current_query] = current_semantic_docid\n",
        "\n",
        "# transpose the dictionary\n",
        "transposed_dict = {'document': list(query_to_semantic_docid_map.keys()), 'semantic_id': list(query_to_semantic_docid_map.values())}\n",
        "# Convert the transposed dictionary to a DataFrame\n",
        "train_queries_df = pd.DataFrame(transposed_dict)\n",
        "\n",
        "full_corpus_df['doctype'] = 'document'\n",
        "train_queries_df['doctype'] = 'query'\n",
        "\n",
        "# in order to save memory, take just the first 50 words from each document\n",
        "def shorten_document(row):\n",
        "    document = row['document']\n",
        "    words = document.split()\n",
        "    max_words = min(50, len(words))\n",
        "    document = ' '.join(words[:max_words])\n",
        "\n",
        "  return document\n",
        "\n",
        "full_corpus_df['document'] = full_corpus_df.apply(shorten_document, axis = 1)\n",
        "\n",
        "# the final training dataset will be a merge of all the documents to be indexed and the train queries\n",
        "train_df = pd.concat([full_corpus_df, train_queries_df], ignore_index = True)\n",
        "\n",
        "# shuffle the questions and documents in the dataset\n",
        "train_df = train_df.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "# make sure that there are no null values or duplicates\n",
        "train_df = train_df.dropna()\n",
        "train_df = train_df.drop_duplicates\n",
        "\n",
        "export_pickle(train_df, 'train_df.pkl')"
      ],
      "metadata": {
        "id": "c40juX7QHB-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our proposed Query Generation approach, queries are generated using the *all-t5-base-v1* model from HuggingFace. Each query is then concatenated to the original document.\\\n",
        " We only want to generate queries for documents, not for the already existing training queries, hence we will work on the full_corpus_df and then merge it afain with the train_queries_df."
      ],
      "metadata": {
        "id": "f0ZIHFnw5NBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# initialize query generation pipeline\n",
        "query_pipe = pipeline('text2text-generation', model = 'doc2query/all-t5-base-v1')"
      ],
      "metadata": {
        "id": "3A5fQ_MC5MwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_query(row):\n",
        "    '''\n",
        "    Generates a query for each row of the document and appends it at the beginning.\n",
        "    '''\n",
        "    doc = row['document']\n",
        "    query = query_pipe(doc)[0]['generated_text']\n",
        "    return query + ' ' + doc\n",
        "\n",
        "full_corpus_df['document'] = full_corpus_df.apply(append_query, axis=1)"
      ],
      "metadata": {
        "id": "Qx3OqKnJ7JVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_qg = pd.concat([full_corpus_df, train_queries_df], ignore_index = True)\n",
        "export_pickle(train_df, 'train_df_qg.pkl')"
      ],
      "metadata": {
        "id": "JH408tJg7tbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}